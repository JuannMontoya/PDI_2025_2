# -*- coding: utf-8 -*-
"""Copy of Clase 9 - Class - LiteRT-HFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ugVWAMDKWDLlYcopu9NvFerPHxgdIre
"""

class TFLiteConverter:
    """Clase para convertir y evaluar modelos TFLite con diferentes cuantizaciones"""

    def __init__(self, model, test_dataset, model_name='model'):
        """
        Args:
            model: Modelo de Keras entrenado
            test_dataset: Dataset de prueba (tf.data.Dataset)
            model_name: Nombre base para guardar los modelos
        """
        self.model = model
        self.test_dataset = test_dataset
        self.model_name = model_name
        self.results = {}

    def representative_dataset_gen(self):
        """Generador de dataset representativo para cuantizaci√≥n INT8"""
        # Tomar 100 muestras del dataset de prueba
        for images, _ in self.test_dataset.unbatch().batch(1).take(100):
            yield [tf.cast(images, tf.float32)]

    def convert_to_tflite_float32(self):
        """Convierte el modelo a TFLite Float32 (sin cuantizaci√≥n)"""
        print("\n" + "="*60)
        print("Convirtiendo a TFLite Float32...")
        print("="*60)

        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        # Especificar expl√≠citamente float32 para entrada y salida
        converter.inference_input_type = tf.float32
        converter.inference_output_type = tf.float32
        tflite_model = converter.convert()

        # Guardar modelo
        model_path = f'{self.model_name}_float32.tflite'
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        print(f"Modelo guardado en {model_path}")
        return tflite_model, model_path

    def convert_to_tflite_float16(self):
        """Convierte el modelo a TFLite Float16"""
        print("\n" + "="*60)
        print("Convirtiendo a TFLite Float16...")
        print("="*60)

        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        # Float16 usa float32 para entrada/salida, float16 internamente
        converter.inference_input_type = tf.float32
        converter.inference_output_type = tf.float32
        tflite_model = converter.convert()

        # Guardar modelo
        model_path = f'{self.model_name}_float16.tflite'
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        print(f"Modelo guardado en {model_path}")
        return tflite_model, model_path

    def convert_to_tflite_dynamic_quant(self):
        """Convierte el modelo a TFLite con cuantizaci√≥n din√°mica"""
        print("\n" + "="*60)
        print("Convirtiendo a TFLite con Cuantizaci√≥n Din√°mica...")
        print("="*60)

        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        # Dynamic quant usa float32 para entrada/salida, int8 internamente
        converter.inference_input_type = tf.float32
        converter.inference_output_type = tf.float32
        tflite_model = converter.convert()

        # Guardar modelo
        model_path = f'{self.model_name}_dynamic_quant.tflite'
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        print(f"Modelo guardado en {model_path}")
        return tflite_model, model_path

    def convert_to_tflite_int8(self):
        """Convierte el modelo a TFLite INT8 (cuantizaci√≥n completa)"""
        print("\n" + "="*60)
        print("Convirtiendo a TFLite INT8...")
        print("="*60)

        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = self.representative_dataset_gen
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        # INT8 usa uint8 para entrada y salida
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        tflite_model = converter.convert()

        # Guardar modelo
        model_path = f'{self.model_name}_int8.tflite'
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        print(f"Modelo guardado en {model_path}")
        return tflite_model, model_path

    def evaluate_tflite_model(self, model_path, model_type):
        """
        Eval√∫a un modelo TFLite

        Args:
            model_path: Ruta al modelo .tflite
            model_type: Tipo de modelo ('float32', 'float16', 'dynamic', 'int8')
        """
        print(f"\nEvaluando modelo {model_type}...")

        # Cargar modelo TFLite
        interpreter = tf.lite.Interpreter(model_path=model_path)
        interpreter.allocate_tensors()

        # Obtener detalles de entrada/salida
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        # Obtener dtype de entrada y salida
        input_dtype = input_details[0]['dtype']
        output_dtype = output_details[0]['dtype']

        print(f"  Input dtype: {input_dtype}")
        print(f"  Output dtype: {output_dtype}")

        # Obtener informaci√≥n del modelo
        model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
        num_ops = len(interpreter.get_tensor_details())

        # Preparar para evaluaci√≥n
        correct_predictions = 0
        top5_correct = 0
        total_samples = 0
        inference_times = []
        total_loss = 0.0

        # Evaluar en todo el dataset de prueba
        for images, labels in self.test_dataset:
            for i in range(images.shape[0]):
                image = images[i:i+1].numpy()
                label = labels[i].numpy()

                # Preprocesar seg√∫n dtype de entrada del modelo
                if input_dtype == np.uint8:
                    # Para INT8: convertir [0, 1] a uint8 [0, 255]
                    # El modelo internamente maneja scale y zero_point
                    image = (image * 255.0).astype(np.uint8)
                elif input_dtype == np.float32:
                    # Para Float32/Float16/Dynamic: mantener como float32 [0, 1]
                    image = image.astype(np.float32)
                elif input_dtype == np.float16:
                    # Para Float16 (si se especific√≥): convertir a float16
                    image = image.astype(np.float16)
                else:
                    # Fallback: usar float32
                    image = image.astype(np.float32)

                # Inferencia con medici√≥n de tiempo
                start_time = time.perf_counter()
                interpreter.set_tensor(input_details[0]['index'], image)
                interpreter.invoke()
                output = interpreter.get_tensor(output_details[0]['index'])
                end_time = time.perf_counter()

                inference_times.append((end_time - start_time) * 1000)  # ms

                # Postprocesar salida seg√∫n dtype de salida del modelo
                if output_dtype == np.uint8:
                    # Para INT8: convertir uint8 [0, 255] a float [0, 1]
                    # El modelo internamente ya aplic√≥ scale y zero_point
                    output = output.astype(np.float32) / 255.0
                elif output_dtype == np.float32:
                    # Para Float32/Float16/Dynamic: ya est√° en float32
                    output = output.astype(np.float32)
                elif output_dtype == np.float16:
                    # Para Float16 (si se especific√≥): convertir a float32
                    output = output.astype(np.float32)
                else:
                    # Fallback: convertir a float32
                    output = output.astype(np.float32)

                # El modelo ya tiene softmax, output son probabilidades
                probs = output[0]

                # Calcular loss (sparse categorical crossentropy)
                epsilon = 1e-7
                loss = -np.log(np.clip(probs[label], epsilon, 1.0))
                total_loss += loss

                # Top-1 accuracy
                predicted_label = np.argmax(probs)
                if predicted_label == label:
                    correct_predictions += 1

                # Top-5 accuracy
                top5_predictions = np.argsort(probs)[-5:]
                if label in top5_predictions:
                    top5_correct += 1

                total_samples += 1

        # Calcular m√©tricas
        accuracy = correct_predictions / total_samples
        top5_accuracy = top5_correct / total_samples
        avg_loss = total_loss / total_samples
        avg_inference_time = np.mean(inference_times)
        std_inference_time = np.std(inference_times)

        # Guardar resultados
        self.results[model_type] = {
            'model_path': model_path,
            'model_size_mb': model_size,
            'num_ops': num_ops,
            'input_dtype': str(input_dtype),
            'output_dtype': str(output_dtype),
            'accuracy': accuracy,
            'top5_accuracy': top5_accuracy,
            'loss': avg_loss,
            'avg_inference_time_ms': avg_inference_time,
            'std_inference_time_ms': std_inference_time,
            'total_samples': total_samples
        }

        print(f"Evaluaci√≥n completa: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, "
              f"Top-5={top5_accuracy:.4f}, Tiempo Promedio={avg_inference_time:.2f}ms")

        return self.results[model_type]

    def evaluate_keras_model(self):
        """Eval√∫a el modelo Keras original"""
        print("\n" + "="*60)
        print("Evaluando Modelo Keras Original...")
        print("="*60)

        # Evaluar con evaluate (devuelve loss y m√©tricas)
        eval_results = self.model.evaluate(self.test_dataset, verbose=0)
        # eval_results = [loss, accuracy, top5_accuracy, ...]
        loss = eval_results[0]
        accuracy = eval_results[1] if len(eval_results) > 1 else 0.0
        top5_accuracy = eval_results[2] if len(eval_results) > 2 else 0.0

        # Medir tiempo de inferencia
        inference_times = []
        total_samples = 0

        for images, _ in self.test_dataset:
            for i in range(images.shape[0]):
                image = images[i:i+1]

                start_time = time.perf_counter()
                _ = self.model(image, training=False)
                end_time = time.perf_counter()

                inference_times.append((end_time - start_time) * 1000)  # ms
                total_samples += 1

        avg_inference_time = np.mean(inference_times)
        std_inference_time = np.std(inference_times)

        # Obtener tama√±o del modelo (aproximado)
        self.model.save('temp_model.h5')
        model_size = os.path.getsize('temp_model.h5') / (1024 * 1024)  # MB
        os.remove('temp_model.h5')

        # Contar operaciones (capas)
        num_ops = sum([1 for layer in self.model.layers])

        self.results['keras_original'] = {
            'model_path': 'N/A (Keras)',
            'model_size_mb': model_size,
            'num_ops': num_ops,
            'input_dtype': 'float32',
            'output_dtype': 'float32',
            'accuracy': accuracy,
            'top5_accuracy': top5_accuracy,
            'loss': loss,
            'avg_inference_time_ms': avg_inference_time,
            'std_inference_time_ms': std_inference_time,
            'total_samples': total_samples
        }

        print(f"Evaluaci√≥n completa: Loss={loss:.4f}, Accuracy={accuracy:.4f}, "
              f"Top-5={top5_accuracy:.4f}, Tiempo Promedio={avg_inference_time:.2f}ms")

        return self.results['keras_original']

    def convert_all(self):
        """Convierte el modelo a todos los formatos"""
        # Modelo original Keras
        self.evaluate_keras_model()

        # Float32
        _, float32_path = self.convert_to_tflite_float32()
        self.evaluate_tflite_model(float32_path, 'float32')

        # Float16
        _, float16_path = self.convert_to_tflite_float16()
        self.evaluate_tflite_model(float16_path, 'float16')

        # Dynamic Quantization
        _, dynamic_path = self.convert_to_tflite_dynamic_quant()
        self.evaluate_tflite_model(dynamic_path, 'dynamic_quant')

        # INT8
        _, int8_path = self.convert_to_tflite_int8()
        self.evaluate_tflite_model(int8_path, 'int8')

    def print_comparison_table(self):
        """Imprime tabla comparativa de todos los modelos"""
        print("\n" + "="*140)
        print("TABLA COMPARATIVA DE MODELOS")
        print("="*140)
        print(f"{'Tipo':<18} {'Tama√±o (MB)':<13} {'Ops':<6} {'In/Out dtype':<20} {'Loss':<10} {'Acc':<10} "
              f"{'Top-5':<10} {'Tiempo (ms)':<13} {'Desv (ms)':<12}")
        print("-"*140)

        order = ['keras_original', 'float32', 'float16', 'dynamic_quant', 'int8']

        for model_type in order:
            if model_type in self.results:
                r = self.results[model_type]
                dtype_str = f"{r['input_dtype']}/{r['output_dtype']}"
                print(f"{model_type:<18} {r['model_size_mb']:<13.2f} {r['num_ops']:<6} "
                      f"{dtype_str:<20} {r['loss']:<10.4f} {r['accuracy']:<10.4f} "
                      f"{r['top5_accuracy']:<10.4f} {r['avg_inference_time_ms']:<13.2f} "
                      f"{r['std_inference_time_ms']:<12.2f}")

        print("="*140)

        # Comparaciones adicionales
        print("\nCONCLUSIONES CLAVE:")
        print("-"*140)

        if 'keras_original' in self.results and 'int8' in self.results:
            size_reduction = (1 - self.results['int8']['model_size_mb'] /
                            self.results['keras_original']['model_size_mb']) * 100
            speed_improvement = (self.results['keras_original']['avg_inference_time_ms'] /
                               self.results['int8']['avg_inference_time_ms'])
            accuracy_drop = (self.results['keras_original']['accuracy'] -
                           self.results['int8']['accuracy']) * 100
            loss_increase = (self.results['int8']['loss'] -
                           self.results['keras_original']['loss'])

            print(f"INT8 vs Keras Original:")
            print(f"  ‚Ä¢ Reducci√≥n de tama√±o: {size_reduction:.1f}%")
            print(f"  ‚Ä¢ Mejora de velocidad: {speed_improvement:.2f}x")
            print(f"  ‚Ä¢ Ca√≠da de accuracy: {accuracy_drop:.2f}%")
            print(f"  ‚Ä¢ Incremento de loss: {loss_increase:.4f}")

        print("="*140)

converter = TFLiteConverter(model, test_loader.get_dataset(), 'my_classification_model')
converter.convert_all()
converter.print_comparison_table()

notebook_login()

"""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"""

HF_USER = "JuannMontoya"
SPACE_NAME = "spaceRoboflow"
REPO_URL = f"https://huggingface.co/spaces/{HF_USER}/{SPACE_NAME}"

!git clone {REPO_URL}

# Commented out IPython magic to ensure Python compatibility.
# %cd {SPACE_NAME}
!git lfs track "*.keras"
!git lfs track "*.h5"
!git lfs track "*.pt"
!git lfs track "*.onnx"
!git lfs track "*.tflite"
!git lfs track "*.bin"
!git lfs track "*.safetensors"

!git add .gitattributes

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

!cp my_classification_model_float16.tflite {SPACE_NAME}/

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {SPACE_NAME}/app.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# import base64
# import numpy as np
# from PIL import Image
# import io
# import ai_edge_litert.interpreter as interpreter
# 
# app = FastAPI(title="AI Edge LiteRT API")
# 
# # Cargar el modelo TFLite una sola vez al iniciar
# MODEL_PATH = "./my_classification_model_float16.tflite"  # Cambia seg√∫n tu modelo (float32, float16, int8, etc.)
# litert_interpreter = interpreter.Interpreter(model_path=MODEL_PATH)
# litert_interpreter.allocate_tensors()
# 
# # Obtener detalles de entrada/salida
# input_details = litert_interpreter.get_input_details()
# output_details = litert_interpreter.get_output_details()
# 
# # Verificar si el modelo usa cuantizaci√≥n INT8
# IS_INT8_MODEL = input_details[0]['dtype'] == np.uint8
# 
# class ImagePayload(BaseModel):
#     image_base64: str
# 
# @app.get("/")
# def home():
#     return {
#         "status": "ok",
#         "message": "API is running! Use POST /predict",
#         "model_info": {
#             "input_shape": input_details[0]['shape'].tolist(),
#             "input_dtype": str(input_details[0]['dtype']),
#             "output_shape": output_details[0]['shape'].tolist(),
#             "output_dtype": str(output_details[0]['dtype']),
#             "quantized": IS_INT8_MODEL
#         }
#     }
# 
# def preprocess_image(img_bytes, target_size=(224, 224)):
#     """
#     Preprocesa la imagen usando NumPy y PIL
# 
#     Args:
#         img_bytes: Bytes de la imagen
#         target_size: Tupla (height, width)
# 
#     Returns:
#         Imagen preprocesada como numpy array
#     """
#     # Decodificar imagen con PIL
#     img = Image.open(io.BytesIO(img_bytes))
# 
#     # Convertir a RGB si es necesario
#     if img.mode != 'RGB':
#         img = img.convert('RGB')
# 
#     # Redimensionar
#     img = img.resize(target_size, Image.BILINEAR)
# 
#     # Convertir a numpy array
#     img_array = np.array(img, dtype=np.float32)
# 
#     # Normalizar a [0, 1]
#     img_array = img_array / 255.0
# 
#     # Expandir dimensiones para batch
#     img_array = np.expand_dims(img_array, axis=0)
# 
#     # Si es modelo INT8, convertir directamente a uint8 [0, 255]
#     # El modelo internamente hace el escalado y zero point
#     if IS_INT8_MODEL:
#         # Volver a escala [0, 255] y convertir a uint8
#         img_array = (img_array).astype(np.uint8)
# 
#     return img_array
# 
# def postprocess_output(output):
#     """
#     Postprocesa la salida del modelo
# 
#     Args:
#         output: Salida raw del modelo
# 
#     Returns:
#         Probabilidades como lista
#     """
#     # Si es modelo INT8, la salida ya est√° en uint8 [0, 255]
#     # El modelo internamente hace el descalado, solo necesitamos
#     # convertir de uint8 a float [0, 1] o [0, 255] dependiendo del caso
#     if IS_INT8_MODEL:
#         # Convertir de uint8 [0, 255] a float [0, 1]
#         output = output.astype(np.float32)
# 
#     # El modelo ya tiene softmax, as√≠ que solo convertir a lista
#     return output[0].tolist()
# 
# @app.post("/predict")
# def predict(payload: ImagePayload):
#     """
#     Endpoint de predicci√≥n
# 
#     Args:
#         payload: JSON con imagen en base64
# 
#     Returns:
#         Predicciones del modelo
#     """
#     try:
#         # Decodificar base64
#         img_bytes = base64.b64decode(payload.image_base64)
# 
#         # Preprocesar imagen
#         img_array = preprocess_image(img_bytes, target_size=(224, 224))
# 
#         # Inferencia con AI Edge LiteRT
#         litert_interpreter.set_tensor(input_details[0]['index'], img_array)
#         litert_interpreter.invoke()
#         output = litert_interpreter.get_tensor(output_details[0]['index'])
# 
#         # Postprocesar salida
#         predictions = postprocess_output(output)
# 
#         # Obtener clase predicha y confianza
#         predicted_class = int(np.argmax(predictions))
#         confidence = float(predictions[predicted_class])
# 
#         return {
#             "prediction": predictions,
#             "predicted_class": predicted_class,
#             "confidence": confidence,
#             "top_5": sorted(
#                 [(i, float(p)) for i, p in enumerate(predictions)],
#                 key=lambda x: x[1],
#                 reverse=True
#             )[:5]
#         }
# 
#     except Exception as e:
#         return {
#             "error": str(e),
#             "status": "failed"
#         }
# 
# @app.get("/health")
# def health_check():
#     """Health check endpoint"""
#     return {"status": "healthy", "model_loaded": True}

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {SPACE_NAME}/requirements.txt
# fastapi
# uvicorn[standard]
# pydantic
# numpy
# pillow
# ai-edge-litert

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {SPACE_NAME}/runtime.txt
# python-3.10

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {SPACE_NAME}/README.md
# ---
# title: ImageClassificationSpace
# emoji: üöÄ
# colorFrom: blue
# colorTo: purple
# sdk: docker
# pinned: false
# ---
# 
# # ClassificationVCAPI
# API en HuggingFace Space con TensorFlow Serving-like pipeline.

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {SPACE_NAME}/Dockerfile
# FROM python:3.10
# 
# # Crear usuario no-root
# RUN useradd -m -u 1000 user
# USER user
# ENV PATH="/home/user/.local/bin:$PATH"
# 
# WORKDIR /app
# 
# COPY --chown=user ./requirements.txt /app/requirements.txt
# RUN pip install --no-cache-dir -r requirements.txt
# 
# COPY --chown=user . /app
# 
# EXPOSE 7860
# 
# CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]

!git config --global user.email "juamontoyara@unal.edu.co"
!git config --global user.name "JuannMontoya"

# Commented out IPython magic to ensure Python compatibility.
# %cd {SPACE_NAME}
!git add .
!git commit -m "ImageClassificationSpace\nAPI en HuggingFace Space con TensorFlow Serving-like pipeline."
!git push

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

import requests
import base64


# 1. Leer imagen
with open("/content/cuchilloycuchara_PDI_1-2/test/images/IMG_20251117_225321_jpg.rf.8fb0eb727e5368461a37924aaf639ef2.jpg", "rb") as f:
    img_bytes = f.read()

# 2. Convertir a base64
img_b64 = base64.b64encode(img_bytes).decode()

payload = {"image_base64": img_b64}

# 3. Enviar request
url = " https://juannmontoya-spaceRoboflow.hf.space/predict"
res = requests.post(url, json=payload, verify=False)

print(res.json())